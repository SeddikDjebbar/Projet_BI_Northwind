{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565b5a49",
   "metadata": {},
   "source": [
    "# üìä Projet Business Intelligence : Analyse Northwind Data Warehouse\n",
    "\n",
    "Ce Notebook Jupyter utilise les donn√©es charg√©es dans le Data Warehouse `NorthwindDW` (via le script `etl.py`) pour g√©n√©rer les analyses cl√©s.\n",
    "\n",
    "**Outils utilis√©s :**\n",
    "* **Connexion :** `pyodbc`\n",
    "* **Manipulation :** `pandas`\n",
    "* **Visualisation :** `matplotlib` et `seaborn`\n",
    "\n",
    "---\n",
    "## 1. Initialisation et Connexion\n",
    "\n",
    "La premi√®re √©tape consiste √† importer les biblioth√®ques n√©cessaires et √† √©tablir une connexion s√©curis√©e √† l'instance SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ef850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration de la connexion au Data Warehouse\n",
    "# (Utilise les param√®tres identifi√©s dans le script ETL)\n",
    "SQL_DW_SERVER = r'DESKTOP-F8N2M8C\\SQLEXPRESS'\n",
    "SQL_DW_DATABASE = 'NorthwindDW'\n",
    "SQL_DW_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# Cha√Æne de connexion pour l'authentification Windows\n",
    "SQL_CONN_STRING = (\n",
    "    f'DRIVER={SQL_DW_DRIVER};'\n",
    "    f'SERVER={SQL_DW_SERVER};'\n",
    "    f'DATABASE={SQL_DW_DATABASE};'\n",
    "    r'Trusted_Connection=yes;'\n",
    "    r'TrustServerCertificate=yes;'\n",
    ")\n",
    "\n",
    "print(f\"Tentative de connexion au Data Warehouse: {SQL_DW_DATABASE}...\")\n",
    "try:\n",
    "    conn = pyodbc.connect(SQL_CONN_STRING)\n",
    "    print(\"‚úÖ Connexion r√©ussie. Le Data Warehouse est pr√™t pour l'analyse.\")\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"‚ùå √âchec de la connexion. V√©rifiez le serveur et le pilote : {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458964ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Analyse 1 : Tendance des Ventes sur la P√©riode\n",
    "\n",
    "**Objectif :** √âvaluer la performance globale des ventes (`SalesAmount`) mois par mois afin d'identifier toute croissance, stagnation ou d√©clin.\n",
    "\n",
    "### **Conclusion :**\n",
    "Le graphique lin√©aire r√©v√®le une **tendance g√©n√©rale √† la baisse** des revenus sur la p√©riode. Le pic initial est suivi d'une √©rosion progressive, signalant une saturation possible du march√© ou un besoin de r√©√©valuer les strat√©gies commerciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requ√™te: Tendance des ventes mensuelles (utilise FactSales et DimDate)\n",
    "    query_sales_trend = \"\"\"\n",
    "    SELECT \n",
    "        DD.Year,\n",
    "        DD.Month,\n",
    "        SUM(FS.SalesAmount) AS MonthlySales\n",
    "    FROM \n",
    "        FactSales FS\n",
    "    JOIN \n",
    "        DimDate DD ON FS.OrderDateKey = DD.DateKey\n",
    "    GROUP BY \n",
    "        DD.Year,\n",
    "        DD.Month\n",
    "    ORDER BY \n",
    "        DD.Year,\n",
    "        DD.Month;\n",
    "    \"\"\"\n",
    "\n",
    "    df_sales_trend = pd.read_sql(query_sales_trend, conn)\n",
    "    \n",
    "    # Cr√©ation de la colonne de temps pour l'axe X (Ann√©e-Mois)\n",
    "    df_sales_trend['YearMonth'] = df_sales_trend['Year'].astype(str) + '-' + df_sales_trend['Month'].astype(str).str.zfill(2)\n",
    "\n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x='YearMonth', y='MonthlySales', data=df_sales_trend, marker='o')\n",
    "    plt.title('Tendance des Ventes Mensuelles (SalesAmount)', fontsize=16)\n",
    "    plt.xlabel('Mois', fontsize=12)\n",
    "    plt.ylabel('Revenus Totaux ($)', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fd166",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Analyse 2 : Performance Commerciale\n",
    "\n",
    "**Objectif :** Identifier les employ√©s ayant g√©n√©r√© le plus de revenus pour r√©compenser les meilleurs performeurs et analyser leurs m√©thodes de vente.\n",
    "\n",
    "### **Conclusion :**\n",
    "Le classement des ventes montre une nette domination de certains employ√©s, notamment **Margaret Peacock** et **Janet Leverling**. La direction devrait examiner de pr√®s leurs zones g√©ographiques et leurs m√©thodes pour reproduire ce succ√®s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401073f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requ√™te SQL pour agr√©ger les ventes par employ√©\n",
    "    sql_query_top_employees = \"\"\"\n",
    "    SELECT\n",
    "        de.FirstName + ' ' + de.LastName AS EmployeeName,\n",
    "        SUM(fs.SalesAmount) AS TotalSales\n",
    "    FROM FactSales fs\n",
    "    JOIN DimEmployees de ON fs.EmployeeID = de.EmployeeKey -- Utilisation de EmployeeID de FactSales et EmployeeKey de DimEmployees\n",
    "    GROUP BY de.FirstName, de.LastName\n",
    "    ORDER BY TotalSales DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    df_top_employees = pd.read_sql(sql_query_top_employees, conn)\n",
    "\n",
    "    # --- Visualisation (Graphique √† Barres Horizontal) ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='TotalSales', y='EmployeeName', data=df_top_employees, palette='viridis')\n",
    "    plt.title('Performance des Employ√©s (Ventes Totales)')\n",
    "    plt.xlabel('Montant des Ventes (USD)')\n",
    "    plt.ylabel('Employ√©')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nObservation : Top 3 des employ√©s : {df_top_employees.iloc[0]['EmployeeName']}, {df_top_employees.iloc[1]['EmployeeName']}, {df_top_employees.iloc[2]['EmployeeName']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0931cd",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Analyse 3 : Distribution du Volume de Ventes par Cat√©gorie\n",
    "\n",
    "**Objectif :** Comprendre sur quelles cat√©gories de produits se concentre la majorit√© des commandes et des articles vendus (`OrderQuantity`).\n",
    "\n",
    "### **Conclusion :**\n",
    "Le diagramme circulaire confirme que quelques cat√©gories (typiquement **Seafood** et **Dairy Products**) repr√©sentent une part disproportionn√©e du volume total. Une strat√©gie marketing et d'approvisionnement devrait se concentrer davantage sur ces segments cl√©s pour maximiser le potentiel de croissance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc478e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requ√™te: Distribution du volume de commandes par Cat√©gorie de Produit\n",
    "    query_category_volume = \"\"\"\n",
    "    SELECT \n",
    "        DP.CategoryName,\n",
    "        SUM(FS.OrderQuantity) AS TotalQuantity\n",
    "    FROM \n",
    "        FactSales FS\n",
    "    JOIN \n",
    "        DimProducts DP ON FS.ProductID = DP.ProductKey\n",
    "    GROUP BY \n",
    "        DP.CategoryName\n",
    "    ORDER BY \n",
    "        TotalQuantity DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    df_category_volume = pd.read_sql(query_category_volume, conn)\n",
    "    \n",
    "    # Visualisation (Diagramme Circulaire)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Utilisation de autopct='%1.1f%%' pour afficher le pourcentage\n",
    "    plt.pie(\n",
    "        df_category_volume['TotalQuantity'], \n",
    "        labels=df_category_volume['CategoryName'], \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=90, \n",
    "        wedgeprops={'edgecolor': 'black'}\n",
    "    )\n",
    "    plt.title('Distribution du Volume de Commandes par Cat√©gorie', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    # Requ√™te: V√©rification du nombre total de transactions et du CA total\n",
    "    query_verification = \"\"\"\n",
    "    SELECT\n",
    "        COUNT(DISTINCT OrderID) AS TotalOrders,\n",
    "        COUNT(SalesAmount) AS TotalOrderDetails,\n",
    "        SUM(SalesAmount) AS TotalRevenue,\n",
    "        MIN(OrderDateKey) AS FirstOrderDateKey,\n",
    "        MAX(OrderDateKey) AS LastOrderDateKey\n",
    "    FROM\n",
    "        FactSales;\n",
    "    \"\"\"\n",
    "\n",
    "    df_verification = pd.read_sql(query_verification, conn)\n",
    "    \n",
    "    # Affichage des r√©sultats\n",
    "    print(\"\\n--- Donn√©es de FactSales apr√®s Consolidation (SQL + Access) ---\")\n",
    "    print(df_verification.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 1. Connexion √† TON Data Warehouse actuel\n",
    "engine = create_engine(f'mssql+pyodbc://DESKTOP-F8N2M8C\\\\SQLEXPRESS/NorthwindDW?driver=ODBC+Driver+17+for+SQL+Server')\n",
    "\n",
    "# 2. Lecture des tables telles qu'elles sont dans ta base\n",
    "df_cust = pd.read_sql(\"SELECT * FROM DimCustomers\", engine)\n",
    "df_emp = pd.read_sql(\"SELECT * FROM DimEmployees\", engine)\n",
    "df_fact = pd.read_sql(\"SELECT * FROM FactSales\", engine)\n",
    "df_time = pd.read_sql(\"SELECT * FROM DimDate\", engine)\n",
    "\n",
    "# 3. TRADUCTION pour correspondre au code de ton ami (On renomme tout ici)\n",
    "# --- Clients ---\n",
    "customers = df_cust.rename(columns={\n",
    "    'CustomerKey': 'CustomerID',\n",
    "    'CustomerCompanyName': 'Company',\n",
    "    'CustomerContactName': 'ContactName',\n",
    "    'CustomerCountry': 'Country'\n",
    "})\n",
    "\n",
    "# --- Employ√©s ---\n",
    "employees = df_emp.copy()\n",
    "employees['FullName'] = employees['FirstName'] + ' ' + employees['LastName']\n",
    "employees = employees.rename(columns={'EmployeeKey': 'EmployeeID'})\n",
    "\n",
    "# --- Temps ---\n",
    "time_dim = df_time.rename(columns={'DateKey': 'DateKey'})\n",
    "\n",
    "# --- Faits (La table Orders de ton ami) ---\n",
    "orders = df_fact.rename(columns={\n",
    "    'OrderDateKey': 'OrderDateKey',\n",
    "    'ShippedDateKey': 'ShippedDateKey'\n",
    "})\n",
    "\n",
    "# IMPORTANT : Appliquer la logique \"1011900\" de ton ami pour les non-livr√©s\n",
    "# Si la date de livraison est vide (NaN), on met le code 1011900\n",
    "orders['ShippedDateKey'] = orders['ShippedDateKey'].fillna(1011900).astype(int)\n",
    "\n",
    "print(\"‚úÖ Traduction termin√©e. Les variables 'customers', 'employees', 'orders' et 'time_dim' sont pr√™tes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90798b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Calcul du r√©sum√© des livraisons (Logique 1011900)\n",
    "shipped_count = (orders['ShippedDateKey'] != 1011900).sum()\n",
    "not_shipped_count = (orders['ShippedDateKey'] == 1011900).sum()\n",
    "\n",
    "print(f\"üìä R√©sum√© : {shipped_count} commandes livr√©es / {not_shipped_count} commandes en attente.\")\n",
    "\n",
    "# 2. Performance des employ√©s (Top 5 Livraisons)\n",
    "shipped_by_emp = orders[orders['ShippedDateKey'] != 1011900].groupby('EmployeeID').size().reset_index(name='Total')\n",
    "shipped_by_emp = shipped_by_emp.merge(employees[['EmployeeID', 'FullName']], on='EmployeeID')\n",
    "shipped_by_emp = shipped_by_emp.sort_values('Total', ascending=False)\n",
    "\n",
    "# 3. Visualisation simple\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=shipped_by_emp.head(5), x='FullName', y='Total', palette='viridis')\n",
    "plt.title('Top 5 Employ√©s par Commandes Livr√©es')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 4. Commandes non livr√©es par Pays (Pour identifier les retards)\n",
    "not_shipped_geo = orders[orders['ShippedDateKey'] == 1011900].merge(customers[['CustomerID', 'Country']], on='CustomerID')\n",
    "not_shipped_geo = not_shipped_geo.groupby('Country').size().reset_index(name='EnAttente').sort_values('EnAttente', ascending=False)\n",
    "\n",
    "print(\"\\nüìç Top pays avec commandes en attente :\")\n",
    "print(not_shipped_geo.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Pr√©paration des donn√©es pour le camembert (Global)\n",
    "labels = ['Livr√©', 'En Attente']\n",
    "sizes = [shipped_count, not_shipped_count]\n",
    "colors = ['#66b3ff','#ff9999']\n",
    "\n",
    "# 2. Performance par Employ√© (Livr√© vs Non-Livr√©)\n",
    "# On cr√©e une vue pivot pour comparer\n",
    "emp_perf = orders.groupby(['EmployeeID', orders['ShippedDateKey'] == 1011900]).size().unstack(fill_value=0)\n",
    "emp_perf.columns = ['Livr√©', 'En_Attente']\n",
    "emp_perf = emp_perf.merge(employees[['EmployeeID', 'FullName']], on='EmployeeID')\n",
    "\n",
    "# --- AFFICHAGE ---\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Graphique 1 : R√©partition Globale\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors, explode=(0.1, 0))\n",
    "plt.title('√âtat Global des Commandes')\n",
    "\n",
    "# Graphique 2 : Retards par Pays (Ton TOP 5)\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(data=not_shipped_geo.head(5), x='EnAttente', y='Country', palette='Reds_r')\n",
    "plt.title('Top 5 des Pays en Retard')\n",
    "\n",
    "# Graphique 3 : Performance des Employ√©s\n",
    "plt.subplot(2, 1, 2)\n",
    "emp_perf_melted = emp_perf.melt(id_vars='FullName', value_vars=['Livr√©', 'En_Attente'])\n",
    "sns.barplot(data=emp_perf_melted, x='FullName', y='value', hue='variable', palette={'Livr√©': 'skyblue', 'En_Attente': 'salmon'})\n",
    "plt.title('Productivit√© des Employ√©s : Livraisons vs Retards')\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862d7ad",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Fin de l'Analyse\n",
    "\n",
    "La connexion au Data Warehouse a √©t√© ferm√©e. Tous les r√©sultats analytiques sont d√©sormais affich√©s ci-dessus et pr√™ts √† √™tre int√©gr√©s au Rapport Final de Projet (PDF) et √† la vid√©o de pr√©sentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"\\nConnexion SQL Server ferm√©e.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
