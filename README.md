# Projet BI ‚Äî Northwind DW üìä

**R√©sum√© :**
Ce projet met en place un pipeline ETL simple pour construire un Data Warehouse (DW) √† partir des sources Northwind (SQL Server + Microsoft Access). Le script principal `scripts/etl.py` extrait, transforme et pr√©pare des tables dimensionnelles et une table de faits. Un notebook d'analyse (`notebooks/analysis_notebook.ipynb`) fournit des visualisations et des v√©rifications sur le DW construit.

---

## Structure du d√©p√¥t üóÇÔ∏è

- `scripts/etl.py` : script ETL (Extraction ‚Üí Transformation ‚Üí (Chargement))
- `notebooks/analysis_notebook.ipynb` : notebook d'analyse et visualisation
- `data/raw/` : export des tables sources brutes (CSV)
- `data/clean/` : tables nettoy√©es pr√™tes √† charger dans le DW (CSV)
- `reports/`, `figures/`, `video/` : livrables et exports

---

## Pr√©requis üîß

- Python 3.8+ recommand√©
- ODBC Driver 17 for SQL Server (pour `pyodbc`)
- Pilote Microsoft Access (pour `.accdb`) si vous utilisez Access
- Packages Python (ex.) :

```powershell
# Cr√©ez et activez un environnement virtuel (Windows PowerShell)
python -m venv venv; .\venv\Scripts\Activate.ps1

# Option A ‚Äî installer depuis `requirements.txt` (recommand√© si pr√©sent)
# Cr√©ez d'abord le fichier requirements si n√©cessaire :
# pip freeze > requirements.txt
pip install -r requirements.txt

# Option B ‚Äî installer directement (si vous n'avez pas de requirements.txt)
pip install -U pip
pip install pandas pyodbc sqlalchemy matplotlib seaborn jupyter

# Commande unique (PowerShell) pour tout faire en une ligne :
# python -m venv venv; .\venv\Scripts\Activate.ps1; pip install -U pip; pip install -r requirements.txt
```

Astuce : vous pouvez cr√©er un `requirements.txt` √† partir des packages ci-dessus.

---

## Configuration de `etl.py` ‚öôÔ∏è

Avant d'ex√©cuter le script, modifiez les variables en haut de `scripts/etl.py` :

- `SQL_SERVER_NAME` : nom de votre instance SQL Server (ex. `DESKTOP-XXXX\SQLEXPRESS`)
- `SQL_DATABASE_NAME` : base source (ex. `Northwind`)
- `ACCESS_FILE_PATH` ou `ACCESS_DB_PATH` : chemin absolu vers le fichier `.accdb` (ex. `r'C:\Users\...\Northwind.accdb'`)
- `RAW_OUTPUT_DIR` : dossier o√π exporter les CSV bruts (par d√©faut `data/raw/`)

Remarque : le script utilise la connexion Windows (Trusted Connection). Si vous avez besoin d'authentification SQL (login/password), adaptez la cha√Æne de connexion.

---

## Ex√©cution du pipeline ETL ‚ñ∂Ô∏è

1. Activez votre environnement virtuel (voir la section Pr√©requis).
2. Assurez-vous que SQL Server et Access sont accessibles depuis votre machine (drivers install√©s).
3. Lancez le script :

```powershell
python scripts\etl.py
```

Comportement attendu :
- Les tables sources sont extraites depuis SQL Server et Access.
- Les exports bruts sont sauvegard√©s dans `data/raw/` (si la partie d'export est activ√©e).
- Les dimensions (DimDate, DimCustomers, DimProducts, DimEmployees, DimShippers) et la table de faits (FactSales) sont construites en m√©moire.
- Le bloc de chargement vers le Data Warehouse (DW) est pr√™t mais comment√© par d√©faut ‚Äî vous pouvez utiliser `sqlalchemy.create_engine` pour charger vers votre cible.

---

## D√©tails du script ETL (fichier : `scripts/etl.py`) üîç

Le script est organis√© en √©tapes s√©quentielles (E ‚Üí T ‚Üí L) ex√©cut√©es lorsque vous lancez `python scripts/etl.py` :

1. Extraction (E)
   - SQL Server : connexion via `pyodbc` et `SQL_CONN_STRING`. Tables extraites : `Orders`, `Order Details`, `Customers`, `Products`, `Categories`, `Employees`, `Shippers`, `Suppliers`.
   - Microsoft Access (optionnel) : si `ACCESS_DB_PATH` est configur√©, le script lit des tables compl√©mentaires (ex. `Customers_Access`, `OrderDetails_Access`) et les stocke dans `data_access`.

2. Export RAW (optionnel)
   - Les DataFrames extraits peuvent √™tre export√©s dans `data/raw/` en CSV (`;` s√©parateur). Contr√¥lez le chemin via `RAW_OUTPUT_DIR`.

3. Consolidation multi-source
   - Si la source Access est disponible, les tables √©quivalentes (Orders, OrderDetails, Customers, Products, Suppliers) sont concat√©n√©es √† la source SQL. Le script donne la priorit√© aux lignes SQL (drop_duplicates keep='first') et supprime les doublons sur cl√©s logiques (ex. `OrderID`, `ProductID`).

4. Transformation (T)
   - DimDate : conversion des dates en cl√© `DateKey` (YYYYMMDD) et cr√©ation des attributs temporels (Year, Quarter, Month, Day, DayName, MonthName).
   - DimCustomers : concat√©nation SQL + Access, normalisation/renommage (`CustomerKey`, `CustomerNotes`, ...), ajout de la colonne `Notes` si absente.
   - DimProducts : consolidation produits + jointure avec `Categories` pour obtenir `CategoryName` et `StandardPrice`.
   - DimEmployees/DimShippers/DimSuppliers : renommages et s√©lection des attributs utiles.
   - FactSales : fusion `OrderDetails` + `Orders`, renommage `UnitPrice`‚Üí`SaleUnitPrice`, calcul `SalesAmount = Quantity * SaleUnitPrice * (1 - Discount)`, cr√©ation des cl√©s de date (`OrderDateKey`, `ShippedDateKey`) et s√©lection finale des colonnes de faits.

5. Export CLEAN
   - Les dimensions et la table de faits sont export√©es dans `data/clean/` en CSV pr√™ts pour chargement ou audit.

6. Chargement (L) vers le Data Warehouse (optionnel, s√©curis√©)
   - Connexion SQLAlchemy via `create_engine()` et `SQL_DW_CONN_STRING`.
   - Chargement des dimensions avec `to_sql(..., if_exists='replace')`.
   - Pour `FactSales`, le script charge d'abord dans `FactSales_Staging` puis ex√©cute un `DELETE` + `INSERT` en production (transactionnel) pour √©viter les incoh√©rences.

Bonnes pratiques, tests & d√©pannage üõ†Ô∏è
- Pour tester uniquement l'extraction : commentez les blocs Transformation/Chargement ou ex√©cutez le script par pas dans un REPL.
- √âvitez d'√©craser une base de production : testez d'abord sur `NorthwindDW` de dev.
- Si Access n'est pas disponible, le script fonctionne en mode SQL-only (consultez les messages d'erreur imprim√©s).
- Pour rendre le script r√©utilisable : envisagez de le refactorer en fonctions et d'ajouter des options CLI (`--skip-load`, `--export-raw`, `--dw-conn`).


---

## Chargement vers le Data Warehouse (optionnel) üèóÔ∏è

Le code contient un emplacement pour cr√©er une connexion SQLAlchemy et charger les DataFrames :

```python
# Exemple (d√©commentez et adaptez) :
from sqlalchemy import create_engine
sql_dw_engine = create_engine('mssql+pyodbc://<SERVER>/<DB>?driver=ODBC+Driver+17+for+SQL+Server')
DimCustomers.to_sql('DimCustomers', sql_dw_engine, if_exists='replace', index=False)
FactSales.to_sql('FactSales', sql_dw_engine, if_exists='replace', index=False)
```

Conseils :
- Testez d'abord sur une base de dev `NorthwindDW` avant d'√©craser une base de production.
- L'import `create_engine` est volontairement pr√©sent et annot√© pour √©viter les avertissements linters si le bloc reste comment√©.

---

## Utilisation du Notebook d'analyse üß™

Le notebook `notebooks/analysis_notebook.ipynb` contient des cellules pour :
- Se connecter au DW (mettez √† jour `SQL_DW_SERVER` et `SQL_DW_DATABASE` dans le notebook si besoin).
- Ex√©cuter des requ√™tes d'agr√©gation sur `FactSales` et les dimensions.
- Produire des graphiques (tendance des ventes, top employ√©s, r√©partition par cat√©gorie, etc.).

Pour l'utiliser :

```powershell
jupyter notebook notebooks\analysis_notebook.ipynb
```

Ou ouvrez le notebook depuis VS Code (extension Jupyter) et ex√©cutez les cellules dans l'ordre.

---

## R√©solution des probl√®mes courants ‚ö†Ô∏è

- Erreur de connexion `pyodbc.Error`: v√©rifiez le nom du serveur, le nom de la base, et que le driver ODBC est install√©.
- Probl√®me avec Access: v√©rifiez l'installation du pilote Access (32 vs 64 bits) et utilisez le chemin absolu vers `.accdb`.
- Avertissement linter ¬´ import 'create_engine' is not accessed ¬ª : l'import est volontaire, il est annot√© dans le code (`# noqa: F401`) car le chargement est optionnel.

---

## Fichiers de sortie üìÅ

- `data/raw/` : tables originales export√©es (CSV)
- `data/clean/` : r√©sultats de transformation (CSV) pr√™ts √† √™tre charg√©s

---


## Auteurs & Licence ‚úçÔ∏è

- Auteur : SeddikDjebbar (adaptations personnelles)

---

Bonne utilisation ! ‚úÖ